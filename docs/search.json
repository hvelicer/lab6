[
  {
    "objectID": "lab6.html",
    "href": "lab6.html",
    "title": "Lab 6: ML Workflows",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.2\n✔ recipes      1.1.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use tidymodels_prefer() to resolve common conflicts.\n\nlibrary(powerjoin)\nlibrary(glue)\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\n\n\n\n\n\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\n\n\n\n\n\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\nlocal_files   &lt;- glue('data/camels_{types}.txt')\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) \ncamels &lt;- power_full_join(camels ,by = 'gauge_id')"
  },
  {
    "objectID": "lab6.html#setting-up-the-lab",
    "href": "lab6.html#setting-up-the-lab",
    "title": "Lab 6: ML Workflows",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.2\n✔ recipes      1.1.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use tidymodels_prefer() to resolve common conflicts.\n\nlibrary(powerjoin)\nlibrary(glue)\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\n\n\n\n\n\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\n\n\n\n\n\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\nlocal_files   &lt;- glue('data/camels_{types}.txt')\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) \ncamels &lt;- power_full_join(camels ,by = 'gauge_id')"
  },
  {
    "objectID": "lab6.html#question-1",
    "href": "lab6.html#question-1",
    "title": "Lab 6: ML Workflows",
    "section": "Question 1",
    "text": "Question 1\nzero_q_freq represents the frequency of days where Q = 0 mm/day (as a %). The Q variable represents daily discharge.\n\nExploratory Data Analysis\n\nlibrary(ggpubr)\n\nggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map()"
  },
  {
    "objectID": "lab6.html#question-2",
    "href": "lab6.html#question-2",
    "title": "Lab 6: ML Workflows",
    "section": "Question 2",
    "text": "Question 2\n\nMaps of the sites\n\nplot1 &lt;- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = aridity)) +\n  scale_color_gradient(low = \"orange\", high = \"dodgerblue\") +\n  labs(x = \"Longitude\", y = \"Latitude\", title = \"Aridity Across the US\") +\n  ggthemes::theme_map()\n\nplot2 &lt;- p2 &lt;- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = p_mean)) +\n  scale_color_gradient(low = \"green\", high = \"purple\") +\n  labs(x = \"Longitude\", y = \"Latitude\", title = \"Mean Daily Precipitation Across the US\" ) + \n  ggthemes::theme_map()\n\nggarrange(plot1, plot2)\n\n\n\n\n\n\n\n\n\n\nModel Preparation\n\ncamels %&gt;%\n  select(aridity, p_mean, q_mean) %&gt;%\n  drop_na() %&gt;% \n  cor()\n\n           aridity     p_mean     q_mean\naridity  1.0000000 -0.7550090 -0.5817771\np_mean  -0.7550090  1.0000000  0.8865757\nq_mean  -0.5817771  0.8865757  1.0000000\n\n\n\n\nVisual EDA\n\n# XY plot of aridity and rainfall\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\", color = \"red\", linetype = 2) +\n  scale_color_viridis_c() +\n  theme_linedraw() + \n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Testing a transformation\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  scale_color_viridis_c() +\n  # Apply log transformations to the x and y axes\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Visualize how log transform benefits the q_mean data\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  scale_color_viridis_c(trans = \"log\") +\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\",\n        # Expand the legend width ...\n        legend.key.width = unit(2.5, \"cm\"),\n        legend.key.height = unit(.5, \"cm\")) + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nModel Building\n\n# Splitting the data\nset.seed(123)\n\ncamels &lt;- camels %&gt;% \n  mutate(logQmean = log(q_mean))\n\ncamels_split &lt;- initial_split(camels, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\n# Preprocessor: recipe\nrec &lt;-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  step_log(all_predictors()) %&gt;%\n  step_interact(terms = ~ aridity:p_mean) %&gt;%\n  step_naomit(all_predictors(), all_outcomes())\n\n# Naive base lm approach\nbaked_data &lt;- prep(rec, camels_train) %&gt;% \n  bake(new_data = NULL)\n\nlm_base &lt;- lm(logQmean ~ aridity * p_mean, data = baked_data)\nsummary(lm_base)\n\n\nCall:\nlm(formula = logQmean ~ aridity * p_mean, data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity        -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean          1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity:p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))\n\n\nCall:\nlm(formula = logQmean ~ aridity + p_mean + aridity_x_p_mean, \n    data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity          -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean            1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity_x_p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n# Correct Version: prep -&gt; bake -&gt; predict\ntest_data &lt;-  bake(prep(rec), new_data = camels_test)\ntest_data$lm_pred &lt;- predict(lm_base, newdata = test_data)\n\n\n\nModel Evaluation: Statistical and Visual\n\nmetrics(test_data, truth = logQmean, estimate = lm_pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n# Linear model\nggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +\n  scale_color_gradient2(low = \"brown\", mid = \"orange\", high = \"darkgreen\") +\n  geom_point() +\n  geom_abline(linetype = 2) +\n  theme_linedraw() + \n  labs(title = \"Linear Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")\n\n\n\n\n\n\n\n# Workflow\nlm_model &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")\n\nlm_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(lm_model) %&gt;%\n  fit(data = camels_train) \n\nsummary(extract_fit_engine(lm_wf))$coefficients\n\n                   Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)      -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity          -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean            1.4843771 0.15511117   9.569762 4.022500e-20\naridity_x_p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\nsummary(lm_base)$coefficients\n\n                 Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)    -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity        -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean          1.4843771 0.15511117   9.569762 4.022500e-20\naridity:p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n# Making predictions\nlm_data &lt;- augment(lm_wf, new_data = camels_test)\ndim(lm_data)\n\n[1] 135  61\n\n# Scatter plot \nmetrics(lm_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\nggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n# Switching it up\nlibrary(baguette)\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nrf_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(rf_model) %&gt;%\n  fit(data = camels_train) \n\n# Predictions\nrf_data &lt;- augment(rf_wf, new_data = camels_test)\ndim(rf_data)\n\n[1] 135  60\n\n# Scatter plot pt. 2\nmetrics(rf_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.592\n2 rsq     standard       0.736\n3 mae     standard       0.367\n\nggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n# Workflowset approach\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nautoplot(wf)\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 4 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     1\n2 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     1\n3 recipe_rand_fore… Prepro… rmse    0.565  0.0249    10 recipe       rand…     2\n4 recipe_rand_fore… Prepro… rsq     0.769  0.0261    10 recipe       rand…     2"
  },
  {
    "objectID": "lab6.html#question-3",
    "href": "lab6.html#question-3",
    "title": "Lab 6: ML Workflows",
    "section": "Question 3",
    "text": "Question 3\n\nBuild a xgboost (engine) regression (mode) model using boost_tree\n\nxgb_mod &lt;- boost_tree(mode = \"regression\",\n                      trees = 1000) %&gt;%\n  set_engine('xgboost')\n\n\n\nBuild a neural network model using the nnet engine from the baguette package using the bag_mlp function\n\nnn_mod &lt;- bag_mlp() %&gt;%\n  set_engine('nnet') %&gt;%\n  set_mode(\"regression\")\n\n# Adding to the above workflow\nxgb_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(xgb_mod) %&gt;%\n  fit(data = camels_train) %&gt;%\n  augment(camels_train)\n  \nnn_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(nn_mod) %&gt;%\n  fit(data = camels_train) %&gt;%\n  augment(camels_train)\n\n\n\nEvaluate the model and compare it to the linear and random forest models\n\nmetrics(xgb_wf, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     0.00292\n2 rsq     standard     1.00   \n3 mae     standard     0.00211\n\nmetrics(nn_wf, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.478\n2 rsq     standard       0.837\n3 mae     standard       0.299\n\nplot1 &lt;- ggplot(xgb_wf, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  labs(title = \"Boosted Tree Model\") +\n  theme_linedraw()\n\nplot2 &lt;- ggplot(nn_wf, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  labs(title = \"Neural Network Model\") +\n  theme_linedraw()\n\nggarrange(plot1, plot2)\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nAs shown in the above plots, the boosted tree model and neural network model produced different results. This is similar to the previous evaluations on the linear regression model and random forest model which also produced different results. I would move forward with the boosted tree model because its results are directly on the 1:1 line."
  },
  {
    "objectID": "lab6.html#question-4-build-your-own",
    "href": "lab6.html#question-4-build-your-own",
    "title": "Lab 6: ML Workflows",
    "section": "Question 4 (Build Your Own)",
    "text": "Question 4 (Build Your Own)\n\nData Splitting\n\n# Set a seed for reproducible\nset.seed(123456)\n\n# Create an initial split with 75% used for training and 25% for testing\nsplit &lt;- initial_split(camels, prop = 0.75)\n\n# Extract your training and testing sets\ntrain_camels &lt;- training(split)\nglimpse(train_camels)\n\nRows: 503\nColumns: 59\n$ gauge_id             &lt;chr&gt; \"01548500\", \"03357350\", \"01451800\", \"02118500\", \"…\n$ p_mean               &lt;dbl&gt; 3.153346, 3.220156, 3.503207, 3.497721, 3.508962,…\n$ pet_mean             &lt;dbl&gt; 2.271782, 2.529923, 2.418851, 2.854343, 2.566778,…\n$ p_seasonality        &lt;dbl&gt; 0.17916389, 0.19294173, 0.10670771, 0.06450132, 0…\n$ frac_snow            &lt;dbl&gt; 0.202088630, 0.070238198, 0.128362666, 0.04137083…\n$ aridity              &lt;dbl&gt; 0.7204355, 0.7856524, 0.6904675, 0.8160581, 0.731…\n$ high_prec_freq       &lt;dbl&gt; 18.10, 22.60, 23.80, 22.35, 21.45, 22.20, 21.55, …\n$ high_prec_dur        &lt;dbl&gt; 1.120743, 1.167959, 1.233161, 1.238227, 1.194986,…\n$ high_prec_timing     &lt;chr&gt; \"jja\", \"jja\", \"son\", \"mam\", \"mam\", \"djf\", \"mam\", …\n$ low_prec_freq        &lt;dbl&gt; 221.00, 253.55, 254.25, 250.15, 250.60, 257.35, 2…\n$ low_prec_dur         &lt;dbl&gt; 3.177570, 4.183993, 4.188633, 4.556466, 4.111567,…\n$ low_prec_timing      &lt;chr&gt; \"son\", \"son\", \"son\", \"son\", \"son\", \"son\", \"son\", …\n$ geol_1st_class       &lt;chr&gt; \"Siliciclastic sedimentary rocks\", \"Mixed sedimen…\n$ glim_1st_class_frac  &lt;dbl&gt; 1.0000000, 0.9184978, 0.9971881, 1.0000000, 1.000…\n$ geol_2nd_class       &lt;chr&gt; NA, \"Carbonate sedimentary rocks\", \"Carbonate sed…\n$ glim_2nd_class_frac  &lt;dbl&gt; 0.00000000, 0.08150219, 0.00281193, 0.00000000, 0…\n$ carbonate_rocks_frac &lt;dbl&gt; 0.000000000, 0.081502189, 0.002811930, 0.00000000…\n$ geol_porostiy        &lt;dbl&gt; 0.2004, 0.1151, 0.1340, 0.0100, 0.0600, 0.1421, 0…\n$ geol_permeability    &lt;dbl&gt; -14.8083, -16.1169, -16.2182, -14.1000, -11.8000,…\n$ soil_depth_pelletier &lt;dbl&gt; 1.0295666, 41.0222222, 1.0370370, 1.0439189, 39.0…\n$ soil_depth_statsgo   &lt;dbl&gt; 1.3391110, 1.5000000, 0.8278332, 1.5000000, 1.500…\n$ soil_porosity        &lt;dbl&gt; 0.4551614, 0.4546479, 0.4528590, 0.4468136, 0.465…\n$ soil_conductivity    &lt;dbl&gt; 1.2217252, 0.9644969, 1.2430216, 0.8378938, 0.778…\n$ max_water_content    &lt;dbl&gt; 0.5712620, 0.6723718, 0.3612221, 0.6873912, 0.700…\n$ sand_frac            &lt;dbl&gt; 29.48293, 28.80686, 32.12866, 31.08818, 21.01718,…\n$ silt_frac            &lt;dbl&gt; 50.45132, 45.55663, 50.37412, 30.83003, 52.10692,…\n$ clay_frac            &lt;dbl&gt; 14.672193, 25.530649, 17.617025, 37.949064, 26.83…\n$ water_frac           &lt;dbl&gt; 0.000000, 0.000000, 0.000000, 0.000000, 0.000000,…\n$ organic_frac         &lt;dbl&gt; 0.000000, 0.000000, 0.000000, 0.000000, 0.000000,…\n$ other_frac           &lt;dbl&gt; 5.47929870, 0.00000000, 0.00000000, 0.00000000, 0…\n$ gauge_lat            &lt;dbl&gt; 41.52174, 39.76171, 40.66176, 36.00056, 39.07033,…\n$ gauge_lon            &lt;dbl&gt; -77.44748, -86.72945, -75.62685, -80.74556, -85.4…\n$ elev_mean            &lt;dbl&gt; 546.71, 258.64, 180.41, 336.82, 264.12, 143.69, 3…\n$ slope_mean           &lt;dbl&gt; 39.14167, 5.84999, 14.89410, 18.17467, 4.26263, 9…\n$ area_gages2          &lt;dbl&gt; 1557.05, 7.75, 135.84, 400.53, 29.72, 578.94, 822…\n$ area_geospa_fabric   &lt;dbl&gt; 1557.98, 26.11, 149.11, 400.53, 32.49, 581.79, 82…\n$ frac_forest          &lt;dbl&gt; 0.9866, 0.4476, 0.4915, 0.7055, 0.6327, 0.9846, 0…\n$ lai_max              &lt;dbl&gt; 4.7800986, 2.2913309, 2.0552600, 3.8514013, 2.880…\n$ lai_diff             &lt;dbl&gt; 4.2352819, 2.0150006, 1.7182899, 3.2155886, 2.546…\n$ gvf_max              &lt;dbl&gt; 0.8767588, 0.7345496, 0.6913127, 0.8151227, 0.784…\n$ gvf_diff             &lt;dbl&gt; 0.5085068, 0.4967552, 0.4217009, 0.4135953, 0.500…\n$ dom_land_cover_frac  &lt;dbl&gt; 0.8623825, 0.6360405, 0.9931744, 0.6948508, 1.000…\n$ dom_land_cover       &lt;chr&gt; \"    Deciduous Broadleaf Forest\", \"    Croplands\"…\n$ root_depth_50        &lt;dbl&gt; 0.1886238, 0.1800000, 0.1800683, 0.1901779, 0.180…\n$ root_depth_99        &lt;dbl&gt; 1.931191, 1.500000, 1.503413, 1.700084, 1.500000,…\n$ q_mean               &lt;dbl&gt; 1.43544360, 1.18148202, 1.75990923, 1.13558931, 1…\n$ runoff_ratio         &lt;dbl&gt; 0.45521289, 0.36690210, 0.50237084, 0.32466552, 0…\n$ slope_fdc            &lt;dbl&gt; 1.72819889, 1.40885118, 1.58160098, 1.14100195, 0…\n$ baseflow_index       &lt;dbl&gt; 0.49416298, 0.32320194, 0.47715838, 0.65134520, 0…\n$ stream_elas          &lt;dbl&gt; 1.1072080, 1.5561283, 1.5483972, 2.4535915, 0.523…\n$ q5                   &lt;dbl&gt; 0.0848496063, 0.0000000000, 0.1354405779, 0.24433…\n$ q95                  &lt;dbl&gt; 5.01241193, 4.10393317, 5.90391243, 2.82083386, 5…\n$ high_q_freq          &lt;dbl&gt; 8.90, 24.65, 10.00, 4.35, 40.05, 24.65, 26.30, 8.…\n$ high_q_dur           &lt;dbl&gt; 2.507042, 2.088983, 2.000000, 1.380952, 2.075130,…\n$ low_q_freq           &lt;dbl&gt; 88.70, 152.05, 73.35, 14.45, 194.85, 139.35, 150.…\n$ low_q_dur            &lt;dbl&gt; 16.425926, 17.680233, 9.168750, 7.810811, 12.8613…\n$ zero_q_freq          &lt;dbl&gt; 0.00000000, 0.10554415, 0.00000000, 0.00000000, 0…\n$ hfd_mean             &lt;dbl&gt; 162.20, 167.10, 153.50, 172.30, 168.05, 145.45, 1…\n$ logQmean             &lt;dbl&gt; 0.361473928, 0.166769599, 0.565262232, 0.12715172…\n\ntest_camels &lt;- testing(split)\nglimpse(test_camels)\n\nRows: 168\nColumns: 59\n$ gauge_id             &lt;chr&gt; \"01055000\", \"01121000\", \"01134500\", \"01139800\", \"…\n$ p_mean               &lt;dbl&gt; 3.494183, 3.719591, 3.467413, 3.303515, 3.420860,…\n$ pet_mean             &lt;dbl&gt; 2.093235, 2.301408, 2.091698, 2.127185, 2.157740,…\n$ p_seasonality        &lt;dbl&gt; 0.167229041, 0.004098324, 0.228672353, 0.22557908…\n$ frac_snow            &lt;dbl&gt; 0.30604885, 0.15422715, 0.28058209, 0.26232897, 0…\n$ aridity              &lt;dbl&gt; 0.5990628, 0.6187263, 0.6032446, 0.6439157, 0.630…\n$ high_prec_freq       &lt;dbl&gt; 19.15, 21.50, 15.85, 19.45, 19.05, 19.95, 20.40, …\n$ high_prec_dur        &lt;dbl&gt; 1.167683, 1.197772, 1.152727, 1.164671, 1.133929,…\n$ high_prec_timing     &lt;chr&gt; \"son\", \"son\", \"jja\", \"jja\", \"jja\", \"son\", \"son\", …\n$ low_prec_freq        &lt;dbl&gt; 224.85, 241.70, 201.10, 225.55, 225.15, 226.35, 2…\n$ low_prec_dur         &lt;dbl&gt; 3.378663, 3.721324, 2.842403, 3.278343, 3.315906,…\n$ low_prec_timing      &lt;chr&gt; \"mam\", \"son\", \"mam\", \"mam\", \"mam\", \"son\", \"son\", …\n$ geol_1st_class       &lt;chr&gt; \"Siliciclastic sedimentary rocks\", \"Metamorphics\"…\n$ glim_1st_class_frac  &lt;dbl&gt; 0.6808434, 1.0000000, 0.6224206, 1.0000000, 0.663…\n$ geol_2nd_class       &lt;chr&gt; \"Metamorphics\", NA, \"Acid plutonic rocks\", NA, \"C…\n$ glim_2nd_class_frac  &lt;dbl&gt; 0.1683631527, 0.0000000000, 0.3693336019, 0.00000…\n$ carbonate_rocks_frac &lt;dbl&gt; 0.000000000, 0.000000000, 0.000000000, 1.00000000…\n$ geol_porostiy        &lt;dbl&gt; 0.1455, 0.0100, 0.0109, 0.0600, 0.0273, 0.0416, 0…\n$ geol_permeability    &lt;dbl&gt; -14.3578, -14.1000, -14.1198, -11.8000, -13.3802,…\n$ soil_depth_pelletier &lt;dbl&gt; 2.1270588, 8.9622642, 4.0740741, 2.5106383, 2.106…\n$ soil_depth_statsgo   &lt;dbl&gt; 1.3696566, 1.5000000, 1.1438600, 1.2302233, 1.348…\n$ soil_porosity        &lt;dbl&gt; 0.4240259, 0.4172702, 0.4392274, 0.4358067, 0.436…\n$ soil_conductivity    &lt;dbl&gt; 2.5500013, 3.0682180, 1.7559531, 1.9362554, 1.876…\n$ max_water_content    &lt;dbl&gt; 0.5470206, 0.5655593, 0.4767668, 0.5179107, 0.566…\n$ sand_frac            &lt;dbl&gt; 53.86201, 55.81395, 42.58599, 45.31480, 44.95952,…\n$ silt_frac            &lt;dbl&gt; 34.73449, 29.27511, 43.33541, 41.82104, 41.25753,…\n$ clay_frac            &lt;dbl&gt; 10.346323, 9.166663, 12.724207, 13.098176, 13.082…\n$ water_frac           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ organic_frac         &lt;dbl&gt; 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.000…\n$ other_frac           &lt;dbl&gt; 0.7780162, 5.7401161, 1.0960866, 0.0000000, 0.532…\n$ gauge_lat            &lt;dbl&gt; 44.64275, 41.84371, 44.51172, 44.09284, 43.71424,…\n$ gauge_lon            &lt;dbl&gt; -70.58878, -72.16897, -71.83731, -72.33565, -72.4…\n$ elev_mean            &lt;dbl&gt; 423.12, 141.54, 450.54, 518.15, 459.80, 325.71, 4…\n$ slope_mean           &lt;dbl&gt; 72.81304, 16.99385, 47.54354, 40.99895, 60.29261,…\n$ area_gages2          &lt;dbl&gt; 250.64, 70.25, 195.13, 22.80, 1790.24, 106.99, 24…\n$ area_geospa_fabric   &lt;dbl&gt; 251.16, 94.70, 209.50, 24.92, 1780.65, 109.26, 24…\n$ frac_forest          &lt;dbl&gt; 0.9916, 0.9652, 0.9963, 1.0000, 0.9813, 0.9970, 0…\n$ lai_max              &lt;dbl&gt; 5.030033, 5.434685, 4.930550, 5.213126, 5.164034,…\n$ lai_diff             &lt;dbl&gt; 4.319226, 4.823193, 4.273607, 4.633122, 4.495081,…\n$ gvf_max              &lt;dbl&gt; 0.8861635, 0.9085610, 0.8862162, 0.8990329, 0.897…\n$ gvf_diff             &lt;dbl&gt; 0.4714315, 0.4976272, 0.4767941, 0.5260267, 0.492…\n$ dom_land_cover_frac  &lt;dbl&gt; 0.5242488, 0.9778111, 0.6038041, 1.0000000, 0.731…\n$ dom_land_cover       &lt;chr&gt; \"    Mixed Forests\", \"    Deciduous Broadleaf For…\n$ root_depth_50        &lt;dbl&gt; 0.2214549, 0.1913313, 0.2134887, 0.1900000, 0.196…\n$ root_depth_99        &lt;dbl&gt; 2.209700, 2.008876, 2.154840, 2.000000, 1.977670,…\n$ q_mean               &lt;dbl&gt; 2.2798975, 1.9583406, 2.0775792, 1.8116828, 1.858…\n$ runoff_ratio         &lt;dbl&gt; 0.6524836, 0.5264936, 0.5991728, 0.5484106, 0.543…\n$ slope_fdc            &lt;dbl&gt; 1.349312, 1.940049, 1.408166, 1.543569, 1.519482,…\n$ baseflow_index       &lt;dbl&gt; 0.4381317, 0.5363064, 0.4875983, 0.6071732, 0.562…\n$ stream_elas          &lt;dbl&gt; 1.3665871, 1.1660444, 1.1078746, 1.7476152, 1.729…\n$ q5                   &lt;dbl&gt; 0.185464951, 0.080101406, 0.238225467, 0.24680367…\n$ q95                  &lt;dbl&gt; 8.441584, 6.025019, 7.096611, 5.579909, 5.983057,…\n$ high_q_freq          &lt;dbl&gt; 16.25, 5.90, 9.05, 2.15, 5.80, 6.70, 9.05, 4.60, …\n$ high_q_dur           &lt;dbl&gt; 2.056962, 1.475000, 2.154762, 1.869565, 1.812500,…\n$ low_q_freq           &lt;dbl&gt; 83.60, 85.00, 46.15, 37.25, 42.00, 62.15, 73.80, …\n$ low_q_dur            &lt;dbl&gt; 8.941176, 9.883721, 6.888060, 7.163462, 10.120482…\n$ zero_q_freq          &lt;dbl&gt; 0.000000000, 0.000000000, 0.000000000, 0.00000000…\n$ hfd_mean             &lt;dbl&gt; 186.05, 156.35, 187.55, 190.20, 181.50, 174.25, 1…\n$ logQmean             &lt;dbl&gt; 0.82413048, 0.67209749, 0.73120339, 0.59425612, 0…\n\n# Build a 10-fold CV dataset as well\ncv_folds &lt;- vfold_cv(train_camels, v = 10)\n\ncv_folds\n\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits           id    \n   &lt;list&gt;           &lt;chr&gt; \n 1 &lt;split [452/51]&gt; Fold01\n 2 &lt;split [452/51]&gt; Fold02\n 3 &lt;split [452/51]&gt; Fold03\n 4 &lt;split [453/50]&gt; Fold04\n 5 &lt;split [453/50]&gt; Fold05\n 6 &lt;split [453/50]&gt; Fold06\n 7 &lt;split [453/50]&gt; Fold07\n 8 &lt;split [453/50]&gt; Fold08\n 9 &lt;split [453/50]&gt; Fold09\n10 &lt;split [453/50]&gt; Fold10\n\n\n\n\nRecipe\n\n# Define a formula you want to use to predict logQmean\nformula &lt;- logQmean ~ p_mean + aridity + high_prec_dur\n\n\nDescribe in words why you are choosing the formula you are. Consult the downloaded PDF for the data to help you make this decision.\nI chose to use aridity, p_mean, and high_prec_dur in my formula because I believe these three factors have significant influence on the mean daily discharge. Aridity (aridity) represents the dryness of an environment and environments that are more arid will in turn have a lower logQmean. Precipitation (p_mean) adds water to the overall discharge system, and going along with that, high_prec_dur will influence the logQmean because more precipitation means more daily discharge.\n\n# Build a recipe that you feel handles the predictors chosen well\ntrain_camels &lt;- na.omit(train_camels)\n\nrec &lt;- recipe(logQmean ~  p_mean + aridity + high_prec_dur, data = train_camels) %&gt;%\n  step_log(all_predictors()) %&gt;%\n  step_interact(terms = ~ aridity:p_mean) %&gt;% \n  step_naomit(all_predictors(), all_outcomes()) %&gt;%\n  step_zv(all_predictors())\n\nrec_prep &lt;- prep(rec, training = train_camels)\nbaked_data &lt;- bake(rec_prep, new_data = NULL)\n\n\n\n\nDefine 3 Models\n\n# Define a random forest model using the rand_forest function. Set the engine to ranger and the mode to regression\nrf_mod2 &lt;- rand_forest() %&gt;%\n  set_engine('ranger') %&gt;%\n  set_mode(\"regression\")\n\n# Define two other models of your choice\nlm_mod2 &lt;- linear_reg() %&gt;%\n  set_engine('lm') %&gt;%\n  set_mode(\"regression\")\n\nxgb_mod2 &lt;- boost_tree() %&gt;%\n  set_engine('xgboost') %&gt;%\n  set_mode(\"regression\")\n\n\n\nWorkflow set()\n\n# With your pre-processing steps and models defined, you can now build a workflow_set object to fit and evaluate your models. This will allow you to compare the performance of different models on the same data.\n\n# Create a workflow object, add the recipe, add the model(s)\nrf_wf2 &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(rf_mod2)\n\nlm_wf2 &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(lm_mod2)\n\nxgb_wf2 &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(xgb_mod2)\n\n# Fit the model to the resamples\nrf_res &lt;- fit_resamples(rf_wf2, resamples = cv_folds)\nlm_res &lt;- fit_resamples(lm_wf2, resamples = cv_folds)\nxgb_res &lt;- fit_resamples(xgb_wf2, resamples = cv_folds)\n\n\n\nEvaluation\n\n# Use autoplot and rank_results to compare the models.\nwf &lt;- workflow_set(list(rec), list(rf_mod2, lm_mod2, xgb_mod2)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nautoplot(wf)\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 6 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.528  0.0263    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.797  0.0252    10 recipe       rand…     1\n3 recipe_linear_reg Prepro… rmse    0.567  0.0266    10 recipe       line…     2\n4 recipe_linear_reg Prepro… rsq     0.773  0.0218    10 recipe       line…     2\n5 recipe_boost_tree Prepro… rmse    0.560  0.0341    10 recipe       boos…     3\n6 recipe_boost_tree Prepro… rsq     0.768  0.0336    10 recipe       boos…     3\n\n\n\nDescribe what model you think is best and why!\nI think the linear regression model (lm_mod2, in the green) is the best because it has the best ranked metrics for RMSE and the second best metrics for RSQ. It has the combined best metrics in both plots.\n\n\n\nExtract and Evaluate\n\n# Now that you found your favorite model, lets see how it does on the test data!\n\n# Build a workflow (not workflow set) with your favorite model, recipe, and training data. Use fit to fit all training data to the model\nwf_final &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(lm_mod2) %&gt;%\n  fit(data = train_camels)\n\n# Use augment to make predictions on the test data\nwf_data_final &lt;- augment(wf_final, new_data = camels_test)\n\n# Create a plot of the observed vs predicted values with clear title, axis labels, and a compelling color scale\nggplot(wf_data_final, aes(x = .pred, y = logQmean, colour = logQmean)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0, color = \"black\", linetype = \"dashed\") +\n  labs(title = \"Observed vs Predicted Values\",\n       x = \"Predicted logQmean\",\n       y = \"Observed logQmean\") +\n  scale_color_viridis_c()\n\n\n\n\n\n\n\n\n\nDescribe what you think of the results!\nLooking at the plot, the results are pretty accurate. The plotted observed vs. predicted values lie relatively close to the 1:1 line, especially towards the higher logQmean values, which means the predictions have a strong accuracy against the observed values."
  }
]